\documentclass[12pt]{article}

\input{4mbapreamble}
\input{4mba4q}
\newcommand{\BeautifulSolution}{{\color{blue}\begin{proof}{\color{magenta}\dots beautifully clear and concise text to be inserted here\dots}\end{proof}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FANCY HEADER AND FOOTER STUFF %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\fancyhf{} % clear all header and footer parameters
%%%\lhead{Student Name: \theblank{4cm}}
%%%\chead{}
%%%\rhead{Student Number: \theblank{3cm}}
%%%\lfoot{\small\bfseries\ifnum\thepage<\pageref{LastPage}{CONTINUED\\on next page}\else{LAST PAGE}\fi}
\lfoot{}
\cfoot{{\small\bfseries Page \thepage\ of \pageref{LastPage}}}
\rfoot{}
\renewcommand\headrulewidth{0pt} % Removes funny header line
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{center}
{\bf Mathematics 4MB3/6MB3 Mathematical Biology\\
\smallskip
2016 ASSIGNMENT \textcolor{blue}{4}}\\
\medskip
\underline{\emph{Group Name}}: \texttt{{\color{blue}The Infective Collective}}\\
\medskip
\underline{\emph{Group Members}}: {\color{blue}Aurora Basinski-Ferris, Michael Chong, Daniel Park, Daniel Presta}
\end{center}

\bigskip
\noindent
\textcolor{blue}{This assignment was due on Wednesday 14 March 2018 at 11:30am.}

\bigskip

\section{Time Series analysis of Recurrent Epidemics}

\begin{enumerate}[(a)]

\item \TSa

\begin{enumerate}[(i)]

\item \TSai

{\color{blue}{Below, we create a function which returns a data frame with a date column and an associated data column. This is done by mutating the initial data frame from the csv file. We string together the year, month, and day that are originally in three separate columns, and then make this string part of the Date class.}}
<<message=FALSE, warning=FALSE>>=
library(tidyverse) #load necessary packages
library(stringr)
read.ymdc <- function(filename) {
  df<- read.csv(filename, skip=6) #skip 6 because 6 lines 
  #at top of csv file that aren't data
  df %>% mutate(date= as.Date(str_c(year,month,day,sep='-'))) %>% 
    select(-year,-month,-day)
} 
@

\item \TSaii

{\color{blue}{Next, we create a function which produces a moving average time plot. We modify the data frame that the user supplies when calling the function such that each row is an average of the following s rows and the prior s rows (if s is the value that the user specifies to compute the moving average over). We also delete the first s rows and the last s rows. By default, the plot is separate from any previous plots that the user may have created, but you may add the plot to an existing plot by inputting `TRUE' in the third argument of the function.}}
<<>>=
time.plot <- function(df, s, add=FALSE, ...) {
  df2 <- df
  for (i in (s+1):(nrow(df2)-s)) {
  df2$cases[i]<-mean(df$cases[(i-s):(i+s)])
  }
  df2$cases[1:s]<-NA
  df2$cases[(nrow(df2)-s):(nrow(df2))] <-NA
  
  if(add) {
    lines(df2$date,df2$cases, ...)
  } else {
  plot(df2$date, df2$cases,type="l", ...)
  }
}
@

\item \TSaiii

{\color{blue}{Finally, we create a function which produces a periodogram on a specific date range of a time series. By default, we have that the begin date is the beginning of the time series and the end date is the end of the time series. However, these can be modified by the user. The function filters the data frame to only include the date range specified by the user. Then it performs a periodogram on that filtered data frame using the commands given in the assignment. Similar to the time.plot function, by default, the plot is separate from any previous plots the user may have created, but the plot may be added to an existing plot by specifiying `TRUE' in the fourth argument of the function.}}
<<>>=
periodogram <-function(df, 
                       start.date=as.Date(df$date[1]),
                       end.date=as.Date(df$date[nrow(df)]), 
                       add=FALSE, ...) {
   df <- df %>%
    filter(date >= start.date & date<=end.date)

s <- spectrum(df$cases, plot = FALSE)
plot((s$freq)^(-1), s$spec, type='l', ...)
}
@

\end{enumerate}

\item \TSb

{\color{blue}{First, we perform analysis on the London time series. Our first step is to plot the time series, as well as a plot smoothed by a 10 week moving average. This smoothed data helps us identify areas of the time series that appear to have similar patterns. Namely, we note that the beginning of the time series until around 1950 appears to have consistent patterns. We also note that the data from around 1950 to 1975 and 1975 until 1994 appear to have similar structure. To investigate these claims, we plot periodograms. Our first periodogram looks at the entire time series (we cut it off at 250 weeks as when we plotted the whole 2500 weeks, we stop seeing patterns after around 200 weeks). This first periodogram tells us that across the whole time series, the most power is at around the 100 week mark, and the next most power is located around the 50 week mark. We note that these roughly correlate to 2 and 1 year cycles. Next, to investigate how these cycles may change over the time series, we plot periodograms for the intervals mentioned earlier which we identified through the initial moving average plot. The plot in the upper right corner shows the periodogram for the period from the beginning of the time series until 1 January 1950. We see that in this time period, the most power is located around the 50 week mark. Next, we plot the time interval from 1 January 1950 until 1 January 1975. Here, we see that the most power is located around the 2 year mark, while there is also a significant amount of power located around the 1 year mark. This tells us that in this period, there are mostly 2 year cycles, with a less significant 1 year cycle. Finally, we look at the end of the time series (from 1 January 1975 until 31 December 1994). In this periodogram, we see that the most power is located around the 140 week mark, with the next most significant power around the 50 week mark. This suggests that in this time period, the most significant cycle is around a three year cycle. However, if we look at the periodogram for the whole time series, we see that that cycle is insignificant overall compared to the 1 year and 2 year cycles. This may be because the three year cycle only appares in the last part of the time series. It also may be because the magnitude of power behind this cycle is much lower than the other cycles in other sections of data.}}
<<>>=
#load london time series using function from 1a(i)
londondata <- read.ymdc('meas_uk__lon_1944-94_wk.csv') 
plot(londondata$date, londondata$cases, xlab="Weeks", ylab="Incidence",
     main ="10 Week Moving Average Plot of Liverpool Data")
#plot using 10 week moving average using function from 1a(ii)
time.plot(londondata,10,add= TRUE, col='red') 

par(mfrow=c(2,2)) #create multiplot. two columns and two rows

periodogram(londondata,
            xlim=c(0,250), 
            main="Entire time series", 
            xlab="Weeks", 
            ylab="Power") 

periodogram(londondata,
            end.date=as.Date('1950-01-01'),
            xlim=c(1,250),
            main="1944-01-07 to 1950-01-01",
            xlab="Weeks", ylab="Power") 

periodogram(londondata,
            as.Date('1950-01-01'),
            as.Date('1975-01-01'),
            xlim=c(1,250), 
            main="1950-01-01 to 1975-01-01",
            xlab="Weeks", ylab="Power") 

periodogram(londondata,
            as.Date('1975-01-01'),
            as.Date('1994-12-31'),
            xlim=c(1,250), 
            main="1975-01-01 to 1994-12-31",
            xlab="Weeks", ylab="Power")  
@

{\color{blue}{Next, we perform analysis on the Liverpool time series. Again, our first step is to plot the time series, as well as a plot smoothed by a 10 week moving average. Based on this smoothed data, we identify intervals of interest again, which will be tweaked and investigated with periodograms. These intervals identified are from the beginning of the data set to 1950, from 1950 until just before 1970, and from just before 1970 until the end of the time series. Our first plot is a periodogram of the whole time series. In this plot, we can see that the patterns across the whole time series aren't as clear as in the London data series. However, there appear to be strong 1 year and 2 year cycles despite some other less significant cycles present. We then investigate the intervals of interest which we identified through visual patterns in the moving average plot. First, we looked at the time interval from the beginning of the time series until 1 January 1950. The periodogram for this time revealed that the the strongest cycle in this time was around 1 year. Next, we found that the strongest cycle in the time period from 1 January 1950 until 1 January 1968 was two years. However, there was also a weak 1 year cycle present. Finally, in the time period from 1 January 1968 until 31 December 1994, we find that the most power is around the 130 week mark. However, there is also a significant amount of power around the 50 week mark.}}
<<>>=
#Perform analysis on the lpl time series
#load liverpool time series
lpldata<-read.ymdc('meas_uk__lpl_1944-94_wk.csv') 
plot(lpldata$date, lpldata$cases, xlab="Weeks", ylab="Incidence",
     main="10 Week Moving Average Plot of Liverpool Data")
time.plot(lpldata,10,add=TRUE, col='red')

par(mfrow=c(2,2)) #create multiplot. two columns and two rows

#periodogram of full time series
periodogram(lpldata, 
            main="Entire timeseries",
            xlim=c(0,300),xlab="Weeks", ylab="Power") 
periodogram(londondata,
            end.date=as.Date('1950-01-01'),
            xlim=c(1,250), 
            main="1944-01-07 to 1950-01-01",
            xlab="Weeks", ylab="Power")

#strongest 2 year cycle. slightly less strong 1 year cycle
periodogram(londondata,as.Date('1950-01-01'),
            as.Date('1968-01-01'),
            xlim=c(1,250), 
            main="1950-01-01 to 1968-01-01",
            xlab="Weeks", ylab="Power") 

#strong 3 year cycle. less strong 1 year cycle
periodogram(londondata,
            as.Date('1968-01-01'),
            as.Date('1994-12-31'),
            xlim=c(1,250), 
            main="1968-01-01 to 1994-12-31",
            xlab="Weeks", ylab="Power") 

@

\end{enumerate}

\section{Stochastic Epidemic Simulations}

\SEintro

\begin{enumerate}[(a)]

\item \SEa

<<>>=
SI.Gillespie <- function(beta = 1, N = 100, I0 = 1, tmax=10) {
  
  # initialize variables
  t <- 0 
  I <- I0
  S <- N - I0
  n <- 1
  
  while(t[n] < tmax & I[n] < N) {
    # calculate event rate
    a <- beta*S[n]*I[n]
    
    # generate uniform random variable
    u <- runif(1)
    
    # time until next event
    dt <- (1/a)*log(1/(1-u))
    
    # record time
    t[n + 1] <- t[n] + dt
    
    # update state
    S[n+ 1] <- S[n] - 1
    I[n + 1] <- I[n] + 1
    
    # increment counter
    n <- n+1
  }
  
  # return time series
  tibble(
      time = t[1:(length(t) - 1)], 
      infected = I[1:(length(I) - 1)], 
      susceptible = S[1:(length(S) - 1)]
  )
  
}
@

\item \SEb

<<>>=
library(deSolve)
# Function to describe 
SI.d <- function(t, y, p) {
  with(as.list(c(y, p)), {
    dI <- beta*(N-I)*I
      
    return(list(c(dI)))
  })
}


  
@

<<>>==
set.seed(8)
test.sizes <- tribble(
  ~pop, ~tmax,
  32, 0.35,
  100, 0.15,
  1000, 0.02,
  10000, 0.0025
)

par(mfrow = c(2, 2))

initial <- c(I = 1)


for (i in 1:4) {
  size <- test.sizes$pop[i]
  tmax <- test.sizes$tmax[i]
  
  timesteps <- seq(0, tmax, length.out = 300)
  
  params <- c(beta = 1, N = size)
  sol <- ode(initial, timesteps, SI.d, params)
  plot(as.data.frame(sol), type = "l", lwd = 3,
       main= str_c("Solution of SI Model for N= ", size))
  
  for (j in 1:30){
    lines(SI.Gillespie(beta = params["beta"], 
                       N = params["N"], 
                       I0 = initial["I"], 
                       tmax = tmax), col = j)
  }
  lines(as.data.frame(sol), type = "l", lwd = 3)
}
@

The Gillespie simulation was run 30 times for each of the 4 population sizes: N = 32 (top left), N = 100 (top right), N = 1000 (bottom left) and N = 10000 (bottom right). Different runs of the simulation are shown in different colors, while the deterministic solution (using the \texttt{deSolve} package) is shown with a thick black line. 

As $N$ increases, we see that the Gillespie simulation becomes smoother, and agrees more closely with the deterministic solution in shape. 

\end{enumerate}

\section{$\R_0$ for smallpox}

\Rintro

\begin{enumerate}[(a)]

  \item \Ra

    {\color{blue}{
Denote the susceptible state by $S$, the incubation state by $E$, and the recovered state by $R$. Furthermore, denote the four infectious stages by $I_1, I_2, I_3$, and $I_4$, respectively, and let the each of the stages represent the four different levels of infectiousness exhibited by infected individuals. For example, the $I_1$ stage corresponds to the prodrom stage, in which infectiousness is rare, while $I_4$ stage represents both the pustules \& scabs stage and the resolving scabs stage, as both of these stages share a low level of infectiousness. Note that all states are repsented by proportions.

Let $\sigma$ represent the per capita rate at which an infected individual develops symptoms and let $\gamma_i$ represent the per capita rate at which an infected individual in stage $i$ progresses to the next stage. In other words, $\gamma_i$ is considered to be the removal rate from stage $I_i$, such that the individual progresses to an infectiousness level exhibited in $I_{i+1}$. Moreover, note that $\gamma_4$ corresponds to the per capita recovery rate. 
Finally, let $\beta_i$ represent the infectiousness (per contact transmission rate) of an infected individual in stage $i$ and $\mu$ represent the per capita natural birth/death rate (they are assumed to be equal). Then, we can write an ODE model for this system as follows:
$$
\begin{aligned}
\frac{dS}{dt} &= \mu (1-S) - (\beta_1 I_1 + \beta_2 I_2 + \beta_3 I_3 + \beta_4 I_4) S\\
\frac{dE}{dt} &= (\beta_1 I_1 + \beta_2 I_2 + \beta_3 I_3 + \beta_4 I_4) S - (\sigma + \mu) E \\
\frac{dI_1}{dt} &= \sigma E - (\gamma_1 + \mu) I_1\\
\frac{dI_2}{dt} &= \gamma_1 I_1 - (\gamma_2 + \mu) I_1\\
\frac{dI_3}{dt} &= \gamma_2 I_2 - (\gamma_3 + \mu) I_1\\
\frac{dI_4}{dt} &= \gamma_3 I_3 - (\gamma_4 + \mu) I_1\\
\frac{dR}{dt} &= \gamma_1 I_4 - \mu R\\
\end{aligned}
$$
}}
  \item \Rb

{\color{blue}{
In order for an infected individual to infect a susceptible individual, it must survive the incubation period and become infectious. 
Then, we can think of $\R_0$ as the expected number of secondary cases caused by a typical infected individual, should they survive the incubation period and each of the following stages. This expression for $\R_0$ represents a sum of the basic reproductive numbers from each stage, such that the final number of secondary cases caused by the typical infective individual is merely equal to the number of infections they caused at each stage.
For example, the contribution of infection from the first stage would be equivalent to $\R_0$ of an SEIR model. We denote this value as $\R_{0,1}$:
$$
\beta_1 \times \frac{\sigma}{\sigma + \mu} \times \frac{1}{\gamma_1 + \mu}
$$
where $\beta_1/(\gamma_1 + \mu)$ represent the average number of infections that occur in stage 1 and $\sigma/(\sigma+\mu)$ is the probability that an infected individual does not die before the incubation period is over.
In order for infection to occur in stage $i>1$, an infected individual must not die from natural mortality before reaching stage $i$. Then, the contribution of infection during the second stage would be defined as $\R_{0,2}$, such that
$$
\beta_2 \times \frac{\sigma}{\sigma + \mu} \times \frac{\gamma_1}{\gamma_1 + \mu} \times \frac{1}{\gamma_2 + \mu}.
$$
Note that we now have $\gamma_1/(\gamma_1+\mu)$ to account for probability the of progressing to stage 2 without dying from natural causes while in stage 1.
Likewise, we can do a similar computation for all other stages. Summing each of our values for $\R_{0,i}$, we ultimately obtain
$$
\begin{aligned}
\R_0 &= \beta_1 \times \frac{\sigma}{\sigma + \mu} \times \frac{1}{\gamma_1 + \mu}\\
&+ \beta_2 \times \frac{\sigma}{\sigma + \mu} \times \frac{\gamma_1}{\gamma_1 + \mu} \times \frac{1}{\gamma_2 + \mu}\\
&+ \beta_3 \times \frac{\sigma}{\sigma + \mu} \times \frac{\gamma_1}{\gamma_1 + \mu} \times \frac{\gamma_2}{\gamma_2 + \mu} \times \frac{1}{\gamma_3 + \mu}\\
&+ \beta_4 \times \frac{\sigma}{\sigma + \mu} \times \frac{\gamma_1}{\gamma_1 + \mu} \times \frac{\gamma_2}{\gamma_2 + \mu} \times \frac{\gamma_3}{\gamma_3 + \mu} \times \frac{1}{\gamma_4 + \mu}.
\end{aligned}
$$
}}

  \item \Rc
{\color{blue} {
For this particular system, we have
$$
\mathcal{F} = \begin{pmatrix}
(\beta_1 I_1 + \beta_2 I_2 + \beta_3 I_3 + \beta_4 I_4) S\\
0\\
0\\
0\\
0
\end{pmatrix}, \mathcal{V} = \begin{pmatrix}
(\sigma + \mu) E \\
- \sigma E + (\gamma_1 + \mu) I_1\\
- \gamma_1 I_1 + (\gamma_2 + \mu) I_1\\
- \gamma_2 I_2 + (\gamma_3 + \mu) I_1\\
- \gamma_3 I_3 + (\gamma_4 + \mu) I_1\\
\end{pmatrix}
$$
Linearizing at the disease free equilibrium, we have
$$
F = \begin{pmatrix}
0 & \beta_1 & \beta_2 & \beta_3 & \beta_4\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
\end{pmatrix},
V = \begin{pmatrix}
(\sigma + \mu) & 0 & 0 & 0 & 0\\
- \sigma & \gamma_1 + \mu & 0 & 0 & 0\\
0 & - \gamma_1 & \gamma_2 + \mu & 0 & 0\\
0 & 0 & - \gamma_2 & \gamma_3 + \mu & 0\\
0 & 0 & 0 & - \gamma_3 & \gamma_4 + \mu\\
\end{pmatrix}
$$
Inverting matrix $V$ (by hand; steps not presnted), we observe:
$$
V^{-1} = \begin{pmatrix}
\frac{1}{\sigma + \mu} & 0 & 0 & 0 & 0\\
\frac{\sigma}{(\sigma + \mu)(\gamma_1 + \mu)} & \frac{1}{\gamma_1 + \mu} & 0 & 0 & 0\\
\frac{\sigma \gamma_1}{(\sigma + \mu)(\gamma_1 + \mu)(\gamma_2 + \mu)} & \frac{\gamma_1}{(\gamma_1 + \mu)(\gamma_2 + \mu)} & \frac{1}{\gamma_2 + \mu} & 0 & 0\\
\frac{\sigma \gamma_1 \gamma_2}{(\sigma + \mu)(\gamma_1 + \mu)(\gamma_2 + \mu)(\gamma_3 + \mu)} & \frac{\gamma_1 \gamma_2}{(\gamma_1 + \mu)(\gamma_2 + \mu)(\gamma_3 + \mu)} & \frac{\gamma_2}{(\gamma_2 + \mu)(\gamma_3 + \mu)} & \frac{1}{\gamma_3 + \mu} & 0\\
\frac{\sigma \gamma_1 \gamma_2 \gamma_3}{(\sigma + \mu)(\gamma_1 + \mu)(\gamma_2 + \mu)(\gamma_3 + \mu)(\gamma_4  +\mu)} & \frac{\gamma_1 \gamma_2 \gamma_3}{(\gamma_1 + \mu)(\gamma_2 + \mu)(\gamma_3 + \mu)(\gamma_4  +\mu)} & \frac{\gamma_2 \gamma_3}{(\gamma_2 + \mu)(\gamma_3 + \mu)(\gamma_4  +\mu)} & \frac{\gamma_3}{(\gamma_3 + \mu)(\gamma_4  +\mu)} & \frac{1}{\gamma_4  +\mu}\\
\end{pmatrix}
$$
It is clear that matrix $FV^{-1}$ consists of 0 entries except its first row.
Hence, its eigenvalues are on its diagonal, four of which are zero. The only non-zero entry on the diagonal is the first column entry of the first row, which is equal to the previously derived $\R_0$ value:
$$
\begin{aligned}
&\beta_1 \times \frac{\sigma}{\sigma + \mu} \times \frac{1}{\gamma_1 + \mu}\\
&+ \beta_2 \times \frac{\sigma}{\sigma + \mu} \times \frac{\gamma_1}{\gamma_1 + \mu} \times \frac{1}{\gamma_2 + \mu}\\
&+ \beta_3 \times \frac{\sigma}{\sigma + \mu} \times \frac{\gamma_1}{\gamma_1 + \mu} \times \frac{\gamma_2}{\gamma_2 + \mu} \times \frac{1}{\gamma_3 + \mu}\\
&+ \beta_4 \times \frac{\sigma}{\sigma + \mu} \times \frac{\gamma_1}{\gamma_1 + \mu} \times \frac{\gamma_2}{\gamma_2 + \mu} \times \frac{\gamma_3}{\gamma_3 + \mu} \times \frac{1}{\gamma_4 + \mu}\\
\end{aligned}
$$
Therefore, the derivation of $\R_0$ using the next generation method is consistent with the derivation of $\R_0$ from a biological argument.
}}

  \item \Rd

{\color{blue} {    
Note that for unaltered small pox, the time scale of disease is much shorter than average life span of a person. Then, we can approximate $\R_0$ by assuming that $\mu \approx 0$. The expression for $\R_0$ thus becomes
$$
\R_0 \approx \frac{\beta_1}{\gamma_1} + \frac{\beta_2}{\gamma_2} + \frac{\beta_3}{\gamma_3} + \frac{\beta_4}{\gamma_4}
$$
The alteration causes the early rash stage to be twice as long and so $\gamma_2^{-1}$ changes from 4 days to 8 days.
It is evident that increasing $\gamma_2^{-1}$ will lead to increase in $\R_0$.

Provided that infectiousness during the early rash stage is extreme, we can assume that at least half of the infection occurs during this stage. In the worst case scenario, all infections occur during the early rash stage. These assumptions are reasonable given that disease-induced death can occur in later stages, and so there would be little contribution to infection.
Based on these assumptions, we have that
$$
2.5 < \frac{\beta_2}{\gamma_{2,\tiny{\textrm{original}}}} < 5.
$$
Since altering doubles $\gamma_2^{-1}$, we get
$$
5 < \frac{\beta_2}{\gamma_{2,\tiny{\textrm{altered}}}} < 10.
$$
This translates to
$$
7.5 < \R_{0, \tiny{\textrm{altered}}} <10.
$$
}}
  \item \qRe

{\color{blue} {
We highly recommend that the CDC begin preparing for the smallpox attack immediately. Given that smallpox has been altered to have twice as long of an early rash stage, we expect $\R_0$ to increase by at least a factor of 1.5, and that is in the best-case scenario. In the worst case scenario, the $\R_0$ value corresponding to the altered smallpox will be twice as large as that of the unaltered smallpox, corresponding to the following range: $7.5 < \R_0 < 10$.
This means that the final size of an epidemic will be almost 100\% if the attack is successful.
}}

\end{enumerate}

\smpxnathistfig

\bigskip
\centerline{\bf--- END OF ASSIGNMENT ---}

\bigskip
Compile time for this document:
\today\ @ \thistime

\end{document}
